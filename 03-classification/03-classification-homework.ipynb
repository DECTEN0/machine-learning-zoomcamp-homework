{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fbd9d92",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b40ae",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this homework, we will use the lead scoring dataset Bank Marketing dataset. Download it from [here](https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv).\n",
    "\n",
    "Or you can do it with `wget`:\n",
    "\n",
    "```\n",
    "wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\n",
    "```\n",
    "\n",
    "In this dataset our desired target for classification task will be `converted` variable - has the client signed up to the platform or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654b7fa8-06bc-43aa-ad0c-56dd3b90c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfac6169-fc60-4e78-a894-769e4ec2499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1dadf-c9a3-4605-9fe1-8f9350155e03",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "* Check if the missing values are presented in the features.\n",
    "* If there are missing values:\n",
    "    * For caterogiral features, replace them with 'NA'\n",
    "    * For numerical features, replace with with 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8f8a6fb-404b-4cca-948d-05c280b29089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_source</th>\n",
       "      <th>industry</th>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>location</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>lead_score</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>79450.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social_media</td>\n",
       "      <td>retail</td>\n",
       "      <td>1</td>\n",
       "      <td>46992.0</td>\n",
       "      <td>employed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>events</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>5</td>\n",
       "      <td>78796.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>australia</td>\n",
       "      <td>3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>retail</td>\n",
       "      <td>2</td>\n",
       "      <td>83843.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>australia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>referral</td>\n",
       "      <td>education</td>\n",
       "      <td>3</td>\n",
       "      <td>85012.0</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>europe</td>\n",
       "      <td>3</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lead_source    industry  number_of_courses_viewed  annual_income  \\\n",
       "0      paid_ads         NaN                         1        79450.0   \n",
       "1  social_media      retail                         1        46992.0   \n",
       "2        events  healthcare                         5        78796.0   \n",
       "3      paid_ads      retail                         2        83843.0   \n",
       "4      referral   education                         3        85012.0   \n",
       "\n",
       "  employment_status       location  interaction_count  lead_score  converted  \n",
       "0        unemployed  south_america                  4        0.94          1  \n",
       "1          employed  south_america                  1        0.80          0  \n",
       "2        unemployed      australia                  3        0.69          1  \n",
       "3               NaN      australia                  1        0.87          0  \n",
       "4     self_employed         europe                  3        0.62          1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33c84516-aa92-4bbe-b50a-85abc39fc32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lead_source                 128\n",
       "industry                    134\n",
       "number_of_courses_viewed      0\n",
       "annual_income               181\n",
       "employment_status           100\n",
       "location                     63\n",
       "interaction_count             0\n",
       "lead_score                    0\n",
       "converted                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if the missing values are presented in the features.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4004d9fe-d9ee-4293-b2f3-549febb4b1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lead_source                  object\n",
       "industry                     object\n",
       "number_of_courses_viewed      int64\n",
       "annual_income               float64\n",
       "employment_status            object\n",
       "location                     object\n",
       "interaction_count             int64\n",
       "lead_score                  float64\n",
       "converted                     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80f9de10-48cf-4d6e-b234-67128375e104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lead_source', 'industry', 'employment_status', 'location']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = list(df.select_dtypes(include=\"object\").columns)\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d5acf96-bfb5-424c-b2e9-d53ae361d930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number_of_courses_viewed',\n",
       " 'annual_income',\n",
       " 'interaction_count',\n",
       " 'lead_score',\n",
       " 'converted']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_columns = list(df.select_dtypes(exclude=\"object\").columns)\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e96e95a-bbeb-4f1c-a5e7-e89d37b14633",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    df[col] = df[col].fillna(\"NA\")\n",
    "for num in numerical_columns:\n",
    "    df[num] = df[num].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3366f5bb-6f8b-4200-b2e4-dc71ea4e5d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lead_source                 0\n",
       "industry                    0\n",
       "number_of_courses_viewed    0\n",
       "annual_income               0\n",
       "employment_status           0\n",
       "location                    0\n",
       "interaction_count           0\n",
       "lead_score                  0\n",
       "converted                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a031aa-5821-436a-bfdf-b3bd0b3a438f",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the most frequent observation (mode) for the column `industry`?\n",
    "\n",
    "- `NA`\n",
    "- `technology`\n",
    "- `healthcare`\n",
    "- `retail`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8bfe638c-2416-4b92-92b3-5106170b66d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "industry\n",
       "retail           203\n",
       "finance          200\n",
       "other            198\n",
       "healthcare       187\n",
       "education        187\n",
       "technology       179\n",
       "manufacturing    174\n",
       "NA               134\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"industry\"].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001cfb09-26bc-47d8-ae14-75ae5d5d567a",
   "metadata": {},
   "source": [
    "The most frequent observation  for column `industry`is **retail** with **203** values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6d02db-3dfc-42f2-b1fe-77f491153412",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Create the [correlation matrix](https://www.google.com/search?q=correlation+matrix) for the numerical features of your dataset. \n",
    "In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "\n",
    "What are the two features that have the biggest correlation?\n",
    "\n",
    "- `interaction_count` and `lead_score`\n",
    "- `number_of_courses_viewed` and `lead_score`\n",
    "- `number_of_courses_viewed` and `interaction_count`\n",
    "- `annual_income` and `interaction_count`\n",
    "\n",
    "Only consider the pairs above when answering this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e951e601-a9df-4c4b-91d1-0fb5ddd5650d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>lead_score</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>-0.023565</td>\n",
       "      <td>-0.004879</td>\n",
       "      <td>0.435914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annual_income</th>\n",
       "      <td>0.009770</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>0.015610</td>\n",
       "      <td>0.053131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interaction_count</th>\n",
       "      <td>-0.023565</td>\n",
       "      <td>0.027036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>0.374573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead_score</th>\n",
       "      <td>-0.004879</td>\n",
       "      <td>0.015610</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.193673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted</th>\n",
       "      <td>0.435914</td>\n",
       "      <td>0.053131</td>\n",
       "      <td>0.374573</td>\n",
       "      <td>0.193673</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          number_of_courses_viewed  annual_income  \\\n",
       "number_of_courses_viewed                  1.000000       0.009770   \n",
       "annual_income                             0.009770       1.000000   \n",
       "interaction_count                        -0.023565       0.027036   \n",
       "lead_score                               -0.004879       0.015610   \n",
       "converted                                 0.435914       0.053131   \n",
       "\n",
       "                          interaction_count  lead_score  converted  \n",
       "number_of_courses_viewed          -0.023565   -0.004879   0.435914  \n",
       "annual_income                      0.027036    0.015610   0.053131  \n",
       "interaction_count                  1.000000    0.009888   0.374573  \n",
       "lead_score                         0.009888    1.000000   0.193673  \n",
       "converted                          0.374573    0.193673   1.000000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[numerical_columns].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb714c-fe37-4b22-b370-beb949e2f0a2",
   "metadata": {},
   "source": [
    "From the options above, `annual_income` and `interaction_count` have the highest correlation of **0.027036**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a943d-2d3a-49b2-b489-81a115d865a3",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "\n",
    "* Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
    "* Use Scikit-Learn for that (the `train_test_split` function) and set the seed to `42`.\n",
    "* Make sure that the target value `y` is not in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a745e2d3-8985-430f-909d-20b75ddbae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Split full train and test set\n",
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "#Split train and validation set\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n",
    "\n",
    "#Reset the indices\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "#Set the y_train, y_val and y_test aside\n",
    "y_train = df_train.converted.values\n",
    "y_val = df_val.converted.values\n",
    "y_test = df_test.converted.values\n",
    "\n",
    "#Drop the targets from the dfs\n",
    "del df_train['converted']\n",
    "del df_val['converted']\n",
    "del df_test['converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2e0c460-9787-4bce-87f3-d7b67c4d704c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(876, 293, 293)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf97f42-051b-4ff0-8538-9e232ba3def7",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "* Calculate the mutual information score between `y` and other categorical variables in the dataset. Use the training set only.\n",
    "* Round the scores to 2 decimals using `round(score, 2)`.\n",
    "\n",
    "Which of these variables has the biggest mutual information score?\n",
    "  \n",
    "- `industry`\n",
    "- `location`\n",
    "- `lead_source`\n",
    "- `employment_status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a857aab-23e4-44d3-8c53-9e607f7c75d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lead_source          0.03\n",
       "industry             0.01\n",
       "employment_status    0.01\n",
       "location             0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import mutual_info_score from sklearn\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "#Create a function to calc the mutual_info_score\n",
    "def mutual_info_converted_score(series):\n",
    "    return mutual_info_score(series, df_full_train.converted)\n",
    "\n",
    "#Apply the function to the teaining data on categorical variables only, rounded to 2dp\n",
    "mutual_info = round(df_full_train[categorical_columns].apply(mutual_info_converted_score),2)\n",
    "\n",
    "#Sort the results\n",
    "mutual_info.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab40d261-01fa-4b92-bdb4-2bc9306c529a",
   "metadata": {},
   "source": [
    "From the sorted reults above, lead_source has the highest mutual_info_score:**0.03**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5787109e-f352-4376-9633-1c1e34c72ebe",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "* Now let's train a logistic regression.\n",
    "* Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
    "* Fit the model on the training dataset.\n",
    "    - To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "    - `model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)`\n",
    "* Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
    "\n",
    "What accuracy did you get?\n",
    "\n",
    "- 0.64\n",
    "- 0.74\n",
    "- 0.84\n",
    "- 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c4d489e-1931-4345-9745-f4e36a862b78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m numerical = \u001b[43mnumerical_columns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconverted\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m numerical\n",
      "\u001b[31mValueError\u001b[39m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "numerical_columns.remove('converted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e932246c-d4b9-4505-8674-bd940a2316b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n"
     ]
    }
   ],
   "source": [
    "print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89f7022d-9b3d-417e-a806-519db7afbca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import DictVectorizer from sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#Inscitanciate DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "#Convert the training dataset to dictionary\n",
    "train_dict = df_train[categorical_columns + numerical_columns].to_dict(orient='records')\n",
    "\n",
    "#Fit and transform to one hot encode the training set\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "#Convert the validation dataset to dictionary\n",
    "val_dict = df_val[categorical_columns + numerical_columns].to_dict(orient='records')\n",
    "\n",
    "#Transform to one hot encode the validation set\n",
    "X_val = dv.transform(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d70bde4c-0f3c-4553-a02b-ed2ef939ac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "#Import the LogisticRegression algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Insatiente the model\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "\n",
    "#Fit the model to training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on validation dataset\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "converted_decision = (y_pred >= 0.5)\n",
    "accuracy = (y_val == converted_decision).mean()\n",
    "print(f\"Validation accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e13ef7-0a36-4579-a05d-616787a95db9",
   "metadata": {},
   "source": [
    "### Question 5 \n",
    "\n",
    "* Let's find the least useful feature using the *feature elimination* technique.\n",
    "* Train a model using the same features and parameters as in Q4 (without rounding).\n",
    "* Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "* For each feature, calculate the difference between the original accuracy and the accuracy without the feature. \n",
    "\n",
    "Which of following feature has the smallest difference?\n",
    "\n",
    "- `'industry'`\n",
    "- `'employment_status'`\n",
    "- `'lead_score'`\n",
    "\n",
    "> **Note**: The difference doesn't have to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22a58dcc-310f-46d5-bd9b-60b8d2ee447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6996587030716723\n"
     ]
    }
   ],
   "source": [
    "#Import DictVectorizer from sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#Inscitanciate DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "#Convert the training dataset to dictionary\n",
    "train_dict = df_train[categorical_columns + numerical_columns].to_dict(orient='records')\n",
    "\n",
    "#Fit and transform to one hot encode the training set\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "#Convert the validation dataset to dictionary\n",
    "val_dict = df_val[categorical_columns + numerical_columns].to_dict(orient='records')\n",
    "\n",
    "#Transform to one hot encode the validation set\n",
    "X_val = dv.transform(val_dict)\n",
    "\n",
    "#Import the LogisticRegression algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Insatiente the model\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "\n",
    "#Fit the model to training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on validation dataset\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "converted_decision = (y_pred >= 0.5)\n",
    "base_accuracy = (y_val == (model.predict_proba(X_val)[:, 1] >= 0.5)).mean()\n",
    "print(f\"Validation accuracy: {base_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84d1078b-73b7-44f5-aa49-3a621b7cebbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industry: change_accuracy = 0.000000\n",
      "employment_status: change_accuracy = 0.003413\n",
      "lead_score: change_accuracy = 0.000000\n",
      "\n",
      "Least useful feature: industry\n"
     ]
    }
   ],
   "source": [
    "base_accuracy = (y_val == (model.predict_proba(X_val)[:, 1] >= 0.5)).mean()\n",
    "# Store differences\n",
    "feature_diffs = {}\n",
    "\n",
    "for feature in ['industry', 'employment_status', 'lead_score']:\n",
    "    # Drop this feature from training and validation\n",
    "    idx_to_keep = [i for i, name in enumerate(dv.get_feature_names_out()) if not name.startswith(feature + '=')]\n",
    "\n",
    "    X_train_new = X_train[:, idx_to_keep]\n",
    "    X_val_new = X_val[:, idx_to_keep]\n",
    "\n",
    "    # Train new model\n",
    "    model_new = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    model_new.fit(X_train_new, y_train)\n",
    "\n",
    "    # Compute accuracy without this feature\n",
    "    y_pred_new = model_new.predict_proba(X_val_new)[:, 1]\n",
    "    accuracy_new = (y_val == (y_pred_new >= 0.5)).mean()\n",
    "\n",
    "    # Record difference\n",
    "    diff = base_accuracy - accuracy_new\n",
    "    feature_diffs[feature] = diff\n",
    "    print(f\"{feature}: change_accuracy = {diff:.6f}\")\n",
    "\n",
    "# Find feature with smallest difference\n",
    "least_useful = min(feature_diffs, key=feature_diffs.get)\n",
    "print(f\"\\nLeast useful feature: {least_useful}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e999ab8-7c1d-43ad-a051-9cea8cbb53e1",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "* Now let's train a regularized logistic regression.\n",
    "* Let's try the following values of the parameter `C`: `[0.01, 0.1, 1, 10, 100]`.\n",
    "* Train models using all the features as in Q4.\n",
    "* Calculate the accuracy on the validation dataset and round it to 3 decimal digits.\n",
    "\n",
    "Which of these `C` leads to the best accuracy on the validation set?\n",
    "\n",
    "- 0.01\n",
    "- 0.1\n",
    "- 1\n",
    "- 10\n",
    "- 100\n",
    "\n",
    "> **Note**: If there are multiple options, select the smallest `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1267c9a6-0155-47fc-a71c-a294f2e8e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.01   Validation accuracy: 0.700\n",
      "C: 0.1    Validation accuracy: 0.700\n",
      "C: 1      Validation accuracy: 0.700\n",
      "C: 10     Validation accuracy: 0.700\n",
      "C: 100    Validation accuracy: 0.700\n"
     ]
    }
   ],
   "source": [
    "# Define the range of C values to test\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Store results\n",
    "accuracies = {}\n",
    "\n",
    "for C in C_values:\n",
    "    model_reg = LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42)\n",
    "    model_reg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model_reg.predict_proba(X_val)[:, 1]\n",
    "    val_accuracy = (y_val == (y_pred >= 0.5)).mean()\n",
    "\n",
    "    accuracies[C] = round(val_accuracy, 3)\n",
    "    print(f\"C: {C:<6} Validation accuracy: {val_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d25baf-379f-4bba-8bb7-af20bd432552",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://courses.datatalks.club/ml-zoomcamp-2025/homework/hw03\n",
    "* If your answer doesn't match options exactly, select the closest one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
